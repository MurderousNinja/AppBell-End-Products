{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyOug1+Rof/ihgcGqStxw2E9"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","source":["pip install deepface"],"metadata":{"id":"dpt7IRDpLuuM"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VIDhF83OLVQI"},"outputs":[],"source":["import cv2\n","from deepface import DeepFace\n","import time"]},{"cell_type":"code","source":["model = DeepFace.build_model(\"VGG-Face\")"],"metadata":{"id":"aDUMPcwlP0ZV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["cap = cv2.VideoCapture(0)  # 0 for the default camera, you can specify a different camera if needed\n","\n","while True:\n","    ret, frame = cap.read()  # Read a frame from the camera feed\n","\n","    try:\n","        # Display the frame with recognized face\n","        cv2.imshow('Live Feed', frame)\n","\n","        # Perform face recognition on the frame\n","        result = DeepFace.analyze(frame, actions=[\"emotion\", \"age\", \"gender\", \"race\"], detector_backend=\"retinaface\", align=True, silent=True)\n","\n","        # Print the recognized person's name (you'll need a database of known faces for this)\n","        print(result[0]['dominant_emotion'])\n","        print(result[0]['dominant_race'])\n","        print(result[0]['dominant_gender'])\n","        print('-' * 200)\n","\n","        if cv2.waitKey(1) & 0xFF == ord('q'):\n","            break\n","    except:\n","        pass\n","\n","cap.release()\n","cv2.destroyAllWindows()"],"metadata":{"id":"eJZKHoT6P08U"},"execution_count":null,"outputs":[]}]}